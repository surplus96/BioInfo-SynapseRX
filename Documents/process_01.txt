
### **1단계: 문헌·데이터 수집 및 지식 그래프 구축 (구현 계획)**

#### **프로세스 상세 설명**

이 단계의 핵심 목표는 **전 세계에 흩어져 있는 비정형(Unstructured) 생물학 데이터(논문, 특허 등)를 자동으로 수집하고, AI를 통해 정제하여, 연구자가 쉽게 탐색할 수 있는 상호 연결된 지식 네트워크, 즉 지식 그래프(Knowledge Graph)로 변환**하는 것입니다.

**워크플로우:**

1.  **데이터 수집 (Crawling):** PubMed, Semantic Scholar, Google Patents 등 주요 학술/특허 데이터베이스에 API 또는 웹 크롤러로 접근하여 특정 키워드(예: 'Alzheimer's', 'pancreatic cancer', 'GPCR')와 관련된 최신 문헌과 특허 데이터를 대량으로 수집합니다.
2.  **데이터 전처리 (Preprocessing):** 수집된 데이터는 대부분 PDF 또는 HTML 형태입니다. 여기서 텍스트를 추출하고, 특히 PDF 내의 표나 그림은 OCR(광학 문자 인식) 기술을 사용해 텍스트 데이터로 변환합니다.
3.  **AI 기반 분석 및 요약 (AI Analysis & Summarization):** 추출된 텍스트를 GPT-4o, Claude 3.5와 같은 거대 언어 모델(LLM)에 전달합니다. LLM은 각 문서의 핵심 내용을 요약하고, 주요 키워드(유전자, 질병, 화합물 등)를 자동으로 태깅합니다.
4.  **지식 추출 및 그래프 구축 (Knowledge Extraction & Graph Construction):** 이 단계가 가장 중요합니다. LLM을 사용하여 정제된 텍스트에서 "A라는 유전자는 B라는 질병과 관련 있다 (`GENE`-`ASSOCIATED_WITH`-`DISEASE`)"와 같은 `주체-관계-객체` 형태의 지식(Triplets)을 추출합니다. 이 정보들을 Neo4j와 같은 그래프 데이터베이스에 노드(Node)와 엣지(Edge) 형태로 저장하여 거대한 지식 네트워크를 구축합니다.
5.  **질의응답 및 시각화 (Query & Visualization):** 최종적으로 연구자는 "A 질병 치료에 사용되는 최신 화합물들은 무엇인가?"와 같은 자연어 질문을 던질 수 있습니다. 시스템은 이 질문을 그래프 데이터베이스가 이해할 수 있는 쿼리(Cypher)로 변환하여 지식 그래프에서 답을 찾고, 그 결과를 다시 자연어로 정리하여 제공합니다.

---

#### **필요 기술 스택 및 도구**

| 구분 | 기술/도구 | 역할 |
| :--- | :--- | :--- |
| **개발 언어** | **Python 3.10+** | 전체 파이프라인 개발의 중심 언어 |
| **데이터 수집** | `requests`, `BeautifulSoup4` | 웹 페이지 크롤링 및 HTML 파싱 |
| | `semantic-scholar-api`, `pubmed-lookup` | 학술 데이터베이스 API 클라이언트 라이브러리 |
| **문서 처리** | `PyMuPDF`, `pdfplumber` | PDF 파일에서 텍스트 및 이미지 추출 |
| | `paddleocr` | 이미지 속 텍스트를 인식하는 OCR 라이브러리 (GPU 권장) |
| **LLM 연동** | `langchain`, `llama-index` | LLM 모델 호출, 프롬프트 관리 등 워크플로우 자동화 |
| | `openai`, `anthropic`, `google-generativeai` | 각 LLM 서비스 API와 연동하기 위한 Python SDK |
| **지식 그래프** | **Neo4j** | 데이터를 노드와 관계로 저장하는 그래프 데이터베이스 |
| | `neo4j` (Python Driver) | Python 코드와 Neo4j 데이터베이스 연결 |
| **개발 환경** | `Docker` & `Docker Compose` | Neo4j 등 외부 서비스를 독립된 환경에서 실행 |
| | `Jupyter Notebook` / `VS Code` | 초기 탐색 및 프로토타이핑 |
| | `.env` 파일 관리 (`python-dotenv`) | API 키 등 민감한 정보 관리 |

---

#### **프로젝트 코드 구조 제안**

위 기술 스택을 바탕으로, 확장 가능하고 유지보수가 용이한 모듈형 코드 구조를 다음과 같이 제안합니다. 이 구조에 따라 파일을 생성하며 개발을 진행하겠습니다.

```plaintext
/bio-knowledge-miner/
├── main.py                 # 整個 파이프라인을 실행하는 메인 스크립트
├── config.py               # API 키, DB 접속 정보, 파일 경로 등 설정 관리
├── requirements.txt        # 프로젝트에 필요한 모든 파이썬 라이브러리 목록
├── .env                    # (Git 무시) 실제 API 키와 비밀번호 저장
|
├── data_collection/        # 1. 데이터 수집 모듈
│   ├── __init__.py
│   ├── api_clients.py      # PubMed, Semantic Scholar 등 외부 API 연동 로직
│   └── crawler.py          # 수집 실행 및 데이터 저장 로직
|
├── text_processing/        # 2. 문서 처리 모듈
│   ├── __init__.py
│   ├── pdf_parser.py       # PDF 파일 파싱 (텍스트, 이미지 추출)
│   └── ocr_handler.py      # 이미지에서 OCR 수행
|
├── llm_services/           # 3. LLM 연동 모듈
│   ├── __init__.py
│   ├── summarizer.py       # 문서 요약 및 키워드 태깅
│   └── entity_extractor.py # 텍스트에서 지식(Node, Relationship) 추출
|
└── knowledge_graph/        # 4. 지식 그래프 모듈
    ├── __init__.py
    ├── neo4j_connector.py  # Neo4j 데이터베이스 연결 및 쿼리 실행
    ├── kg_builder.py       # 추출된 지식을 DB에 저장(구축)
    └── graph_rag_query.py  # 자연어 질의를 Cypher 쿼리로 변환 및 응답 생성
```