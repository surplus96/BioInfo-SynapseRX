from . import config
from .data_collection.crawler import run_pubmed_crawler
from .text_processing.pdf_parser import process_all_pdfs_in_directory
from .llm_services.summarizer import summarize_text
from .llm_services.entity_extractor import llm_extract_entities
from .knowledge_graph.kg_builder import ingest_document_entities
import json, os
from datetime import datetime
# global variable to share between steps
extracted_pdf_data_cache = []
current_time = datetime.now().strftime("%Y%m%d")
OUTPUT_JSON = os.path.join(config.EXTRACTION_PATH, f"pdf_entities_summary_{current_time}.json")

def step_1_collect_data():
    """ë…¼ë¬¸ ë° íŠ¹í—ˆ ë°ì´í„° ìˆ˜ì§‘"""
    print("--- Step 1: Collecting data from PubMed ---")
    
    # í¬ë¡¤ë§í•  ê²€ìƒ‰ì–´ì™€ ê°œìˆ˜ ì„¤ì •
    search_queries = [
        "KRAS G12C inhibitors[Title/Abstract]",
        "Sotorasib mechanism of action[Title/Abstract]",
        "KRAS mutations in pancreatic cancer[Title/Abstract]"
    ]
    max_results = 20 # ê° ì¿¼ë¦¬ë‹¹ 20ê°œ
    
    run_pubmed_crawler(search_queries, max_results)
    
    print("Data collection complete.")
    print("-" * 30)

def step_2_process_text():
    """PDF íŒŒì‹± ë° OCR ìˆ˜í–‰"""
    print("--- Step 2: Processing text from documents ---")
    
    # pdfs í´ë”ì— ìˆëŠ” ëª¨ë“  PDF íŒŒì¼ì˜ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
    global extracted_pdf_data_cache
    extracted_pdf_data_cache = process_all_pdfs_in_directory()
    
    # ì´í›„ ë‹¨ê³„ì—ì„œ ì‚¬ìš©í•˜ê¸° ìœ„í•´ ì´ ë°ì´í„°ë¥¼ ë³€ìˆ˜ë¡œ ê°€ì§€ê³  ìˆê±°ë‚˜ íŒŒì¼ë¡œ ì €ì¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    # ì§€ê¸ˆì€ ì¶”ì¶œëœ íŒŒì¼ ê°œìˆ˜ë§Œ ì¶œë ¥í•©ë‹ˆë‹¤.
    if extracted_pdf_data_cache:
        print(f"Successfully extracted text from {len(extracted_pdf_data_cache)} PDF(s).")

    print("Text processing complete.")
    print("-" * 30)

def step_3_extract_entities():
    """LLMì„ ì´ìš©í•´ ì§€ì‹ ì¶”ì¶œ"""
    print("--- Step 3: Extracting knowledge with LLM ---")
    if not extracted_pdf_data_cache:
        print("No extracted PDF data to process.")
        return

    for item in extracted_pdf_data_cache:
        content = item["content"]
        summary = summarize_text(content, max_tokens=500)
        entities = llm_extract_entities(content)
        # ì €ì¥
        item["summary"] = summary
        item["entities"] = entities

        print("\n[SUMMARY]")
        print(summary)
        print("[ENTITIES]")
        print(entities)

    # ëª¨ë“  ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥
    try:
        with open(OUTPUT_JSON, "w", encoding="utf-8") as f:
            json.dump(extracted_pdf_data_cache, f, ensure_ascii=False, indent=2)
        print(f"\nğŸ“ Saved detailed results to {OUTPUT_JSON}")
    except Exception as e:
        print(f"Could not write result json: {e}")

    print("Knowledge extraction complete.")
    print("-" * 30)

def step_4_build_knowledge_graph():
    """ì¶”ì¶œëœ ì§€ì‹ìœ¼ë¡œ ê·¸ë˜í”„ DB êµ¬ì¶•"""
    print("--- Step 4: Building Knowledge Graph ---")
    if not extracted_pdf_data_cache:
        print("No data to ingest into KG")
        return

    for item in extracted_pdf_data_cache:
        entities = item.get("entities")
        if entities:
            ingest_document_entities(entities)

    print("Knowledge Graph build complete.")
    print("-" * 30)

def main():
    """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
    print("ğŸš€ Starting Bio-Knowledge Miner Pipeline...")
    
    step_1_collect_data()
    step_2_process_text()
    step_3_extract_entities()
    step_4_build_knowledge_graph()
    
    print("âœ… Pipeline finished successfully!")

if __name__ == "__main__":
    main() 